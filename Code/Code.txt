#Installing Java 8 & Install Hadoop

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!java -version

!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java
!update-alternatives --set javac /usr/lib/jvm/java-8-openjdk-amd64/bin/javac
!update-alternatives --set jps /usr/lib/jvm/java-8-openjdk-amd64/bin/jps
!java -version

#Finding the default Java path
!readlink -f /usr/bin/java | sed "s:bin/java::"
!apt-get install openssh-server -qq > /dev/null
!service ssh start

!grep Port /etc/ssh/sshd_config

#Creating a new rsa key pair with empty password
!ssh-keygen -t rsa -P "" -f ~/.ssh/id_rsa <<< y

# See id_rsa.pub content
!more /root/.ssh/id_rsa.pub

#Copying the key to autorized keys
!cat $HOME/.ssh/id_rsa.pub > $HOME/.ssh/authorized_keys
#Changing the permissions on the key
!chmod 0600 ~/.ssh/authorized_keys

#Conneting with the local machine
!ssh -o StrictHostKeyChecking=no localhost uptime


#Downloading Hadoop 3.2.3
!wget -q https://archive.apache.org/dist/hadoop/common/hadoop-3.2.3/hadoop-3.2.3.tar.gz

#Untarring the file
!sudo tar -xzf hadoop-3.2.3.tar.gz
#Removing the tar file
!rm hadoop-3.2.3.tar.gz


#Copying the hadoop files to user/local
!cp -r hadoop-3.2.3/ /usr/local/
#-r copy directories recursively

#Adding JAVA_HOME directory to hadoop-env.sh file
!sed -i '/export JAVA_HOME=/a export JAVA_HOME=\/usr\/lib\/jvm\/java-8-openjdk-amd64' /usr/local/hadoop-3.2.3/etc/hadoop/hadoop-env.sh

import os
#Creating environment variables
#Creating Hadoop home variable

os.environ["HADOOP_HOME"] = "/usr/local/hadoop-3.2.3"
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["JRE_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64/jre"
os.environ["PATH"] += f'{os.environ["JAVA_HOME"]}/bin:{os.environ["JRE_HOME"]}/bin:{os.environ["HADOOP_HOME"]}/sbin'

!echo $JRE_HOME

# ---------------------------------------------------------------------------------------

# Hadoop Streaming Using Python

#Creating directory in HDFS
!$HADOOP_HOME/bin/hdfs dfs -mkdir /content/word_count_uud

# ---------------------------------------------------------------------------------------

#Copying the file from local file system to Hadoop distributed file system (HDFS)
!$HADOOP_HOME/bin/hdfs dfs -put -f  /content/UUD.txt  /content/word_count_uud

# ---------------------------------------------------------------------------------------

%%writefile mapper.py

#!/usr/bin/env python

#'#!' is known as shebang and used for interpreting the script

# import sys because we need to read and write data to STDIN and STDOUT
import sys

# reading entire line from STDIN (standard input)
for line in sys.stdin:
  # to remove leading and trailing whitespace
  ###
  ### "sadsadas dsdasda"
  ### sadsadas
  ### sadsadas
  line = line.strip()
  # split the line into words, output data type list
  words = line.split()
  
  # we are looping over the words array and printing the word
  # with the count of 1 to the STDOUT
  for word in words:
    if word in ["(", ")"] :
      continue
    # write the results to STDOUT (standard output);
    # what we output here will be the input for the
    # Reduce step, i.e. the input for reducer.py
    print('%s\t%s' % (word.lower().replace(",", ""), 1))

# ---------------------------------------------------------------------------------------

%%writefile reducer.py

#!/usr/bin/env python

from operator import itemgetter
import sys

current_word = None
current_count = 0
word = None

# read the entire line from STDIN
for line in sys.stdin:
  # remove leading and trailing whitespace
  line = line.strip()
  # splitting the data on the basis of tab we have provided in mapper.py
  word, count = line.split('\t', 1)
  # convert count (currently a string) to int
  try:
    count = int(count)
  except ValueError:
    # count was not a number, so silently
    # ignore/discard this line
    continue

  # this IF-switch only works because Hadoop sorts map output
  # by key (here: word) before it is passed to the reducer
  if current_word == word:
    current_count += count
  else:
    if current_word: #to not print current_word=None
      # write result to STDOUT
      print('%s\t%s' % (current_word, current_count))
    current_count = count
    current_word = word

# do not forget to output the last word if needed!
if current_word == word:
  print('%s\t%s' % (current_word, current_count))

# ---------------------------------------------------------------------------------------

#Testing our MapReduce job locally (Hadoop does not participate here)
!cat /content/UUD.txt | python mapper.py | sort -k1,1 | python reducer.py | head -10
#We apply sorting after the mapper because it is the default operation in MapReduce architecture



# Output
'''
1945	1
abadi	1
adil	2
Allah	1
atas	1
Atas	1
bagi	1
Bahwa	1
bangsa	2
bangsa,	1
'''
# Output

# ---------------------------------------------------------------------------------------

#Changing the permissions of the files
!chmod 777 /content/mapper.py /content/reducer.py
#Setting 777 permissions to a file or directory means that it will be readable, writable and executable by all users

# ---------------------------------------------------------------------------------------

# Remove the existing output directory if it exists
!$HADOOP_HOME/bin/hadoop fs -rm -r /content/word_count_uud/output
#Running MapReduce programs
!$HADOOP_HOME/bin/hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-3.2.3.jar \
  -input /content/word_count_uud/UUD.txt \
  -output /content/word_count_uud/output \
  -mapper "python /content/mapper.py" \
  -reducer "python /content/reducer.py"


# ---------------------------------------------------------------------------------------

#Exploring the created output directory
#part-r-00000 contains the actual ouput
!$HADOOP_HOME/bin/hdfs dfs -ls /content/word_count_uud/output


# ---------------------------------------------------------------------------------------


#Printing out first 50 lines
!$HADOOP_HOME/bin/hdfs dfs -cat /content/word_count_uud/output/part-00000 

# ---------------------------------------------------------------------------------------


!$HADOOP_HOME/bin/hdfs dfs -copyToLocal /content/word_count_uud/output/part-00000 /content/standalone-wordcount.txt

# ---------------------------------------------------------------------------------------

# Output
'''
1945	1
abadi	1
adil	2
allah	1
atas	2
bagi	1
bahwa	1
bangsa	3
bebas	1
beradab	1
berbahagia	1
berdasar	1
berdasarkan	1
berdaulat	1
berkat	1
berkedaulatan	1
berkehidupan	1
bersatu	1
dalam	3
dan	11
darah	1
daripada	1
dasar	2
dengan	6
depan	1
di	1
didorongkan	1
dihapuskan	1
dipimpin	1
disusunlah	1
dunia	2
esa	1
gerbang	1
hak	1
harus	1
hikmat	1
ialah	1
ikut	1
indonesia	12
indonesia.	1
ini	1
itu	4
karena	1
ke	1
keadilan	2
kebangsaan	2
kebijaksanaan	1
kehidupan	1
keinginan	1
kemanusiaan	1
kemerdekaan	5
kemerdekaannya.	1
kemudian	1
kepada	2
kerakyatan	1
kesejahteraan	1
ketertiban	1
ketuhanan	1
kuasa	1
luhur	1
maha	2
maka	3
makmur.	1
melaksanakan	1
melindungi	1
memajukan	1
membentuk	1
mencerdaskan	1
mengantarkan	1
menyatakan	1
merdeka	1
mewujudkan	1
negara	5
oleh	3
pembukaan	1
pemerintah	1
penjajahan	1
perdamaian	1
pergerakan	1
perikeadilan.	1
perikemanusiaan	1
perjuangan	1
permusyawaratan/perwakilan	1
persatuan	1
pintu	1
preambule	1
rakhmat	1
rakyat	4
republik	2
saat	1
sampailah	1
sebab	1
segala	1
segenap	1
selamat	1
seluruh	2
sentausa	1
serta	1
sesuai	1
sesungguhnya	1
sosial	2
suatu	4
supaya	1
susunan	1
tahun	1
telah	1
terbentuk	1
tidak	1
tumpah	1
umum	1
undang-undang	2
untuk	2
yang	11

'''
# Output

